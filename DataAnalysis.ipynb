{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"home\"></a>\n",
    "# Data Analysis with Python\n",
    "<hr \\>\n",
    "\n",
    "## Table of Content\n",
    "1. [Importing Datasets](#imp)\n",
    "2. [Pre-processing Data in Python](#prep)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "4. [Model Development](#mod)\n",
    "5. [Measures for In-Sample Evaluation](#insamp)\n",
    "6. [Model Evaluation](#eva)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"imp\"></a>\n",
    "## Importing  Datasets\n",
    "Data analysis and, in essence data science, helps us unlock the information and insights from raw data to answer our questions. So data analysis plays an important role by helping us to \n",
    "* discover useful information from the data, \n",
    "* answer questions, \n",
    "* and predict the future or the unknown  \n",
    "\n",
    "### Understanding Data\n",
    "#### Python Packages\n",
    "A Python library is a collection of functions and methods that allow you to perform lots of actions without writing any code. The libraries usually contain built in modules providing different functionalities which you can use directly. \n",
    "\n",
    "And there are extensive libraries offering a broad range of facilities. We have divided the Python data analysis libraries into three groups. \n",
    "1. <b>Scientific computing libraries</b>\n",
    "     1.1 <b>Pandas</b> \n",
    "         * offers data structure and tools for effective data manipulation and analysis. \n",
    "         * It provides facts, access to structured data.\n",
    "         * The primary instrument of Pandas is the two dimensional table consisting of column and row labels, which are called a data frame. \n",
    "         * It is designed to provided easy indexing functionality. \n",
    "     1.2 The <b>NumPy</b> library \n",
    "         * uses arrays for its inputs and outputs. \n",
    "         * can be extended to objects for matrices and \n",
    "         * with minor coding changes, developers can perform fast array processing. \n",
    "     1.3 <b>SciPy</b> includes \n",
    "         * functions for some advanced math problems such as: Integrals, differential equations and optimisations\n",
    "         * data visualization  \n",
    "2. <b>Data visualization libraries</B>\n",
    "    These libraries enable you to create graphs, charts and maps. \n",
    "    2.1 </b>Matplotlib package</b> \n",
    "        * is the most well known library for data visualization. \n",
    "        * It is great for making graphs and plots. \n",
    "        * The graphs are customizable. \n",
    "    2.2 <b>Seaborn</b>\n",
    "        * Is based on Matplotlib\n",
    "        * Easy to generate various plots such as heat maps, time series and violin plots.\n",
    "3. <b>Algorithmic libraries</b>\n",
    "    The algorithmic libraries tackles the machine learning tasks from basic to complex. \n",
    "    3.1 <b>Scikit-learn library</b> contains tools statistical modeling, including regression, classification, clustering, and so on. This library is built on NumPy, SciPy and Matplotib.\n",
    "    3.2 <b>Statsmodels</b> is a Python module that allows users to explore data, estimate statistical models, and perform statistical tests.\n",
    "\n",
    "#### Importing and Exporting Data in Python\n",
    "<b>Data acquisition</b> is a process of loading and reading data into notebook from various sources. To read any data using Python's pandas package, there are two important factors to consider, <b>format and file path</b>. \n",
    "* <b>Format</b> is the way data is encoded. Some common encodings are: CSV, JSON, XLSX, HDF ...\n",
    "* <b>Path</b> tells us where the data is stored  \n",
    "\n",
    "In pandas, the <b>read_CSV method</b> can read in files with columns separated by commas into a pandas data frame. \n",
    "1. import pandas \n",
    "2. define a variable with a file path\n",
    "3. use the read_ CSV method to import the data\n",
    "\n",
    "After reading the dataset, it is a good idea to look at the data frame to get a better intuition and to ensure that everything occurred the way you expected. \n",
    "* df prints the entire dataset \n",
    "* df.head(n) shows the first n rows of the data frame. \n",
    "* df.tail(n)shows the bottom end rows of data frame.\n",
    "\n",
    "### Start analysing Data in Python\n",
    "Pandas has several built-in methods that can be used to understand the datatype or features or to look at the distribution of data within the dataset. Using these methods, gives an overview of the dataset and also point out potential issues such as the wrong data type of features which may need to be resolved later on. \n",
    "\n",
    "Data has a variety of types. The main types stored in Pandas' objects are \n",
    "* object, \n",
    "* float, \n",
    "* Int, \n",
    "* datetime\n",
    "\n",
    "The statistical metrics can tell the data scientist if there are mathematical issues that may exist such as extreme outliers and large deviations. \n",
    "\n",
    "To get the quick statistics, we use the describe method (<b>df.describe(inlcude=\"all\")</b>). It returns the number of terms in the column as \n",
    "* count, \n",
    "* average \n",
    "* mean\n",
    "* standard deviation\n",
    "* maximum \n",
    "* minimum\n",
    "* boundary of each of the quartiles\n",
    "\n",
    "By default, the dataframe.describe functions skips rows and columns that do not contain numbers. It is possible to make the describe method worked for object type columns as well. To enable a summary of all the columns, we could add an argument. Include equals all inside the describe function bracket. \n",
    "\n",
    "A different set of statistics is evaluated, like \n",
    "* unique, \n",
    "* top, \n",
    "* frequency  \n",
    "\n",
    "### Accessing Databases with Python\n",
    "The Python code connects to the database using API calls (application programming interface), which is a set of functions that you can call to get access to some type of servers.  \n",
    "The two main concepts in the Python DB-API are <b>connection and query objects</b>. \n",
    "* You use connection objects to connect to a database and manage your transactions. \n",
    "* Cursor objects are used to run queries. You open a cursor object and then run queries. \n",
    "\n",
    "Exercise\n",
    "1. import pandas library\n",
    "    import pandas as pd\n",
    "2. Read the online file by the URL provides above, and assign it to variable \"df\"\n",
    "    other_path = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/auto.csv\"\n",
    "    df = pd.read_csv(other_path, header=None)\n",
    "3. create headers list\n",
    "    headers = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\n",
    "         \"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\n",
    "         \"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\n",
    "         \"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]  \n",
    "    print(\"headers\\n\", headers)  \n",
    "    df.columns = headers  \n",
    "    df.head(10)  \n",
    "4. drop missing values along the column \"price\"\n",
    "    df.dropna(subset=[\"price\"], axis=0)  \n",
    "5. Find the names of the columns  \n",
    "    print(df.columns)\n",
    "6. Save a Dataset\n",
    "    df.to_csv(\"automobile.csv\", index=False)\n",
    "    \n",
    "| Data Formate  | Read           | Save             |\n",
    "| ------------- |:--------------:| ----------------:|\n",
    "| csv           | `pd.read_csv()`  |`df.to_csv()`     |\n",
    "| json          | `pd.read_json()` |`df.to_json()`    |\n",
    "| excel         | `pd.read_excel()`|`df.to_excel()`   |\n",
    "| hdf           | `pd.read_hdf()`  |`df.to_hdf()`     |\n",
    "| sql           | `pd.read_sql()`  |`df.to_sql()`     |\n",
    "| ...           |   ...          |       ...        |\n",
    "\n",
    "7. Data types\n",
    "    df.dtypes or print(df.dtypes)\n",
    "8. Statistics\n",
    "    dataframe.describe()  \n",
    "    df.describe()  \n",
    "    df.describe(include = \"all\")  \n",
    "9. Statistics over specific columns  \n",
    "    df[['length','compression-ratio']].describe()\n",
    "10. Info\n",
    "    dataframe.info, e.g. df.info\n",
    "\n",
    "[Home](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"prep\"></a>\n",
    "# Pre-processing Data in Python\n",
    "\n",
    "It is the process of converting or mapping data from one raw form into another format to make it ready for further analysis. Data preprocessing is often called data cleaning or data wrangling.\n",
    "\n",
    "1. identify and handle missing values. \n",
    "2. data formats, standardize the values into the same format, or unit, or convention \n",
    "3. data normalization (centering and scaling\n",
    "4. Data Binning\n",
    "5. Turning categorical values into numeric variables to make statistical modeling easier\n",
    "\n",
    "### Dealing with missing values\n",
    "Usually missing value in data set appears as question mark and a zero or just a blank cell.\n",
    "Strategies:\n",
    "* check if the person or group that collected the data can go back and find what the actual value is\n",
    "* Drop the missing value\n",
    "    * drop the variable\n",
    "    * drop the data entry\n",
    "* Replace the missing value\n",
    "    * replace it with an average\n",
    "    * replace it by frequency\n",
    "    * repalce it based on other functions\n",
    "* Leave it as missing data\n",
    "\n",
    "To remove data that contains missing values Panda's library has a built-in method called df.dropna(). You'll need to specify axis=0 to drop the <b>rows</b> or axis=1 to drop the <b>columns</b> that contain the missing values. To modify the dataframe, you have to set the parameter inplace=True.\n",
    "\n",
    "To replace missing values like NaNs with actual values, Pandas library has a built-in method called replace \"dataframe.replace(missing_value, new_value)\" which can be used to fill in the missing values with the newly calculated values.\n",
    "\n",
    "### Data formating in Python\n",
    "Data formatting means bringing data into a common standard of expression that allows users to make meaningful comparisons. As a part of dataset cleaning, data formatting ensures the data is consistent and easily understandable.\n",
    "* Convert data -> df[\"city-mpg\"]= 235/df[\"city-mpg\"] plus rename column name df.rename(columns={\"city_mpg: \"city-L/100km\"}, inplace=True)\n",
    "* Convert data type -> df[\"price\"]=df[\"price\"].astype(\"int\")\n",
    "\n",
    "### Data normalisation\n",
    "By making the ranges consistent between variables, normalization enables a fair comparison between the different features, making sure they have the same impact.This normalization can make some statistical analyses easier down the road. By making the ranges consistent between variables, normalization enables a fair comparison between the different features, making sure they have the same impact.\n",
    "\n",
    "Three techniques to nomralise value\n",
    "1. <b>Simple feature scaling</b> just divides each value by the maximum value for that feature:  \n",
    "    df[\"length\"] = df[\"length\"]/df[\"length\"].max \n",
    "2. <b>min-max</b> takes each value X_old subtract it from the minimum value of that feature, then divides by the range of that feature:  \n",
    "    df[\"length\"] = (df[\"length\"]-df[\"length\"].min())/(df[\"length\"].max-df[\"length\"].min())\n",
    "3. <b>z-score or standard score</b>. In this formula for each value you subtract the $\\mu$ which is the average of the feature, and then divide by the standard deviation $\\sigma$ sigma. The resulting values hover around zero:  \n",
    "    df[\"length\"] = (df[\"length\"]-df[\"length\"].mean())/df[\"length\"].std()\n",
    "\n",
    "### Binning in Python\n",
    "Binning is when you group values together into bins. Binning can improve accuracy of the predictive models. In addition, sometimes we use data binning to group a set of numerical values into a smaller number of bins to have a better understanding of the data distribution. \n",
    "\n",
    "In Python we can easily implement the binning: We would like 3 bins of equal binwidth, so we need 4 numbers as dividers that are equal distance apart. \n",
    "1. First we use the numpy function “linspace” to return the array “bins” that contains 4 equally spaced numbers over the specified interval of the price:  \n",
    "bins=np.linspace(min(df[\"price\"]),max(df[\"price\"]),4)\n",
    "2. We create a list “group_names “ that contains the different bin names:  \n",
    "group_name=[\"Low\",\"Medium\",\"High\"]\n",
    "3. We use the pandas function ”cut” to segment and sort the data values into bins. You can then use histograms to visualize the distribution of the data after they’ve been divided into bins:  \n",
    "df[\"price-binned\"] = pd.cut(df[price],bins,lables=group_names,include_lowest=True)\n",
    "\n",
    "### Turning categorical variables into quantitative variables in Python\n",
    "Most statistical models cannot take in objects or strings as input and for model training only take the numbers as inputs. We encode the values by adding new features corresponding to each unique element in the original feature we would like to encode. \n",
    "\n",
    "In Pandas, we can use get_dummies method to convert categorical variables to dummy variables. In Python, transforming categorical variables to dummy variables is simple. The get_dummies method automatically generates a list of numbers, each one corresponding to a particular category of the variable.  \n",
    "\n",
    "pd.get_dummies(df['fuel'])\n",
    "\n",
    "<b>Exercise</b>\n",
    "1. Import the libraries:  \n",
    "    import pandas as pd  \n",
    "    import matplotlib.pylab as plt\n",
    "2. Reading the dataset:  \n",
    "    filename = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/auto.csv\"  \n",
    "    headers = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\n",
    "         \"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\n",
    "         \"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\n",
    "         \"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]  \n",
    "     df = pd.read_csv(filename, names = headers)\n",
    "3. See what the data looks like:\n",
    "    df.head()\n",
    "4. Identify and handle missing data\n",
    "    df.replace(\"?\", np.nan, inplace = True)\n",
    "5. Evaluate for missing data\n",
    "    missing_data = df.isnull()\n",
    "    Count for missing values in each column:\n",
    "        for column in missing_data.columns.values.tolist():  \n",
    "        print(column)  \n",
    "        print (missing_data[column].value_counts())  \n",
    "        print(\"\")  \n",
    "6. Deal with missing data\n",
    "    * drop data\n",
    "    * replace data\n",
    "    replace by mean:  \n",
    "        avg_norm_loss = df[\"normalized-losses\"].astype(\"float\").mean(axis=0)  \n",
    "        df[\"normalized-losses\"].replace(np.nan, avg_norm_loss, inplace=True)  \n",
    "7. Correct data format\n",
    "    List the data formats: df.dtypes()  \n",
    "    Convert data types to proper format: df[[\"bore\", \"stroke\"]] = df[[\"bore\", \"stroke\"]].astype(\"float\")  \n",
    "8. Data standardisation  \n",
    "    df[\"highway-mpg\"] = 235/df[\"highway-mpg\"]  \n",
    "    df.rename(columns={'highway-mpg':'highway-L/100km'}, inplace=True)  \n",
    "    df.head()  \n",
    "9. Data normalisation  \n",
    "    df['length'] = df['length']/df['length'].max()  \n",
    "    df['width'] = df['width']/df['width'].max()  \n",
    "    df['height'] = df['height']/df['height'].max()  \n",
    "    df[[\"length\",\"width\",\"height\"]].head()\n",
    "10. Binning\n",
    "    Plot a histogram to see the best binning options\n",
    "    %matplotlib inline  \n",
    "    import matplotlib as plt  \n",
    "    from matplotlib import pyplot  \n",
    "    plt.pyplot.hist(df[\"horsepower\"])  \n",
    "    \n",
    "    # set x/y labels and plot title  \n",
    "    plt.pyplot.xlabel(\"horsepower\")  \n",
    "    plt.pyplot.ylabel(\"count\")  \n",
    "    plt.pyplot.title(\"horsepower bins\")  \n",
    "    \n",
    "    bins = np.linspace(min(df[\"horsepower\"]), max(df[\"horsepower\"]), 4)  \n",
    "    bins  \n",
    "    group_names = ['Low', 'Medium', 'High']  \n",
    "    df['horsepower-binned'] = pd.cut(df['horsepower'], bins, labels=group_names, include_lowest=True )  \n",
    "    df[['horsepower','horsepower-binned']].head(20)  \n",
    "    df[\"horsepower-binned\"].value_counts()  \n",
    "    \n",
    "    Plot the bins:  \n",
    "    %matplotlib inline  \n",
    "    import matplotlib as plt  \n",
    "    from matplotlib import pyplot  \n",
    "    pyplot.bar(group_names, df[\"horsepower-binned\"].value_counts())  \n",
    "    \n",
    "    # set x/y labels and plot title  \n",
    "    plt.pyplot.xlabel(\"horsepower\")  \n",
    "    plt.pyplot.ylabel(\"count\")  \n",
    "    plt.pyplot.title(\"horsepower bins\")\n",
    "11. Indicator (=dummy) variable  \n",
    "    dummy_variable_1 = pd.get_dummies(df[\"fuel-type\"])  \n",
    "    dummy_variable_1.rename(columns={'fuel-type-diesel':'gas', 'fuel-type-diesel':'diesel'}, inplace=True)  \n",
    "    dummy_variable_1.head()  \n",
    "    \n",
    "    df = pd.concat([df, dummy_variable_1], axis=1)  \n",
    "    df.drop(\"fuel-type\", axis = 1, inplace=True)  \n",
    "    \n",
    "\n",
    "[Home](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='eda'></a>\n",
    "# Exploratory Data Analysis\n",
    "Exploratory data analysis (EDA) is an approach to analyze data in order to \n",
    "* summarize main characteristics of the data, \n",
    "* gain better understanding of the data set, \n",
    "* uncover relationships between different variables, \n",
    "* extract important variables for the problem we're trying to solve.\n",
    "\n",
    "EDA is about about \n",
    "* <b>descriptive statistics</b>, which describe basic features of a data set, and obtain a short summary about the sample and measures of the data\n",
    "* <b>basic of grouping data</b> using GroupBuy, and how this can help to transform our data set\n",
    "* <b>ANOVA</b>, the analysis of variance, a statistical method in which the variation in a set of observations is divided into distinct components\n",
    "* <b>correlation</b> between different variables\n",
    "* <b>advance correlation</b>, where we'll introduce you to various correlations statistical methods namely, Pearson correlation, and correlation heatmaps.\n",
    "\n",
    "### Descriptive Statistics\n",
    "Descriptive statistical analysis helps to describe basic features of a dataset and obtains a short summary about the sample and measures of the data. \n",
    "\n",
    "<b>df.describe()</b>  \n",
    "It shows the mean, the total number of data points, the standard deviation, the quartiles, and the extreme values. Any NaN values are automatically skipped in these statistics.\n",
    "\n",
    "<b>value_counts()</b>  \n",
    "Categorical variables in your dataset can be divided up into different categories or groups and have discrete values.\n",
    "\n",
    "<b>Box plots -- sns.boxplot (x,y,data)</b>\n",
    "Box plots are great way to visualize numeric data, since you can visualize the various distributions of the data. The main features that the box plot shows are the \n",
    "* median of the data, which represents where the middle data point is. \n",
    "* The upper quartile shows where the 75th percentile is. \n",
    "* The lower quartile shows where the 25th percentile is. \n",
    "* The data between the upper and lower quartile represents the interquartile range.\n",
    "With box plots, you can easily spot outliers and also see the distribution and skewness of the data. Box plots make it easy to compare between groups\n",
    "\n",
    "<b>Scatter plot -- plt.scatter(x,y)</b>\n",
    "Each observation in the scatter plot is represented as a point. This plot shows the relationship between two variables. The predictor variable is the variable that you are using to predict an outcome. In this case, our predictor variable is the engine size. The target variable is the variable that you are trying to predict. In this case, our target variable is the price, since this would be the outcome. In a scatter plot, we typically set the predictor variable on the x-axis or horizontal axis, and we set the target variable on the y-axis or vertical axis.  \n",
    "\n",
    "### GroupBy in Python\n",
    "In Pandas, this can be done using the group by method <b>df.groupby()</b>. The group by method is used on \n",
    "* categorical variables, \n",
    "* groups the data into subsets according to the different categories of that variable. \n",
    "* group by a single variable or you can group by multiple variables by passing in multiple variable names. \n",
    "\n",
    "#### Visualising the GroupBy data\n",
    "1. Pivot tables:\n",
    "    Transform this table to a pivot table by using the pivot method. This is similar to what is usually done in Excel spreadsheets:  \n",
    "    df_pivot = df_grp.pivot(indes='X', columns='Y')\n",
    "2. Heat map plot:  \n",
    "    Heat map takes a rectangular grid of data and assigns a color intensity based on the data value at the grid points. It is a great way to plot the target variable over <b>multiple variables</b> and through this get visual clues with the relationship between these variables and the target:  \n",
    "    plt.pcolor(df_pivot, cmap='RdBu')  \n",
    "    plt-colorbar()  \n",
    "    plt.show()\n",
    "    \n",
    "### Correlation\n",
    "Correlation is a statistical metric for measuring to what extent different variables are interdependent. It is important to know that correlation doesn't imply causation.\n",
    "\n",
    "We can use seaborn.regplot to create the scatter plot: sns.regplot(x='x-name',y='y-name',data=df), plt.ylim(0,)\n",
    "\n",
    "### Correlation Statistics\n",
    "One way to measure the strength of the correlation between continuous numerical variable is by using a method called <b>Pearson correlation</b>. Pearson correlation method will give you two values: the correlation coefficient and the P-value. \n",
    "\n",
    "1. Correlation coefficient:\n",
    "    * a value close to +1 implies a large positive correlation, \n",
    "    * a value close to -1 implies a large negative correlation, \n",
    "    * a value close to 0 implies no correlation between the variables. \n",
    "2. P-value will tell us how certain we are about the correlation that we calculated. \n",
    "    * a P-value < 0.001 gives us a <b>strong</b> certainty  \n",
    "    * a P-value > 0.001 and <0.05 gives us <b>moderate</b> certainty. \n",
    "    * a P-value > 0.05 and <0.1 gives us a <b>weak</b> certainty. \n",
    "    * a P-value > 0.1 will give us <b>no</b> certainty of correlation at all.  \n",
    "\n",
    "pearson_coef, p_value = stats.pearsonr(df['x-variable'],df['y-variable'])\n",
    "\n",
    "-> Correlation heatmap\n",
    "The color scheme indicates the Pearson correlation coefficient, indicating the strength of the correlation between two variables. We can see a diagonal line with a dark red color, indicating that all the values on this diagonal are highly correlated. This makes sense because when you look closer, the values on the diagonal are the correlation of all variables with themselves, which will be always 1. This correlation heatmap gives us a good overview of how the different variables are related to one another and, most importantly, how these variables are related to price.\n",
    "\n",
    "### Analysis of Variance (ANOVA)\n",
    "Analyze a categorical variable and see the correlation among different categories.\n",
    "To analyze categorical variables such as the make variable, we can use a method such as the <b>ANOVA</b> method. ANOVA is statistical test that stands for Analysis of Variance. ANOVA can be used to find the correlation between different groups of a categorical variable.  \n",
    "The ANOVA test returns two values, the F-test score and the p-value. \n",
    "1. The <b>F-test</b> calculates the ratio of variation between groups mean, over the variation within each of the sample groups. \n",
    "2. The <b>p-value</b> shows whether the obtained result is statistically significant. \n",
    "Without going too deep into the details, the F-test calculates the ratio of variation between groups means over the variation within each of the sample group means. \n",
    "\n",
    "Exercise:\n",
    "1. Import libraries\n",
    "    import pandas as pd  \n",
    "    import numpy as np  \n",
    "2. Look at the data  \n",
    "    path='https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/automobileEDA.csv'  \n",
    "    df = pd.read_csv(path)  \n",
    "    df.head()\n",
    "3. Install seaborn  \n",
    "    %%capture  \n",
    "    ! pip install seaborn  \n",
    "4. Import visualisation packages  \n",
    "    import matplotlib.pyplot as plt  \n",
    "    import seaborn as sns  \n",
    "    %matplotlib inline \n",
    "5. list the data types  \n",
    "    print(df.dtypes)\n",
    "6. Correlation  \n",
    "    df.corr() or df[['x-value','y-value' ,'z-value','etc-value']].corr()\n",
    "7. Scatter plot  \n",
    "    sns.regplot(x=\"engine-size\", y=\"price\", data=df)  \n",
    "    plt.ylim(0,)  \n",
    "8. Categorical variables with boxplot  \n",
    "    sns.boxplot(x=\"body-style\", y=\"price\", data=df)\n",
    "9. Descriptive Statistics  \n",
    "    df.describe() and with [Objects] df.describe(include=['object'])  \n",
    "10. Value Counts  \n",
    "    df['drive-wheels'].value_counts() or in a frame df['drive-wheels'].value_counts().to_frame()  \n",
    "11. Grouping - what categories \n",
    "    df['drive-wheels'].unique()  \n",
    "12. Create a group with different categories  \n",
    "    df_group_one = df[['drive-wheels','body-style','price']]\n",
    "13. Group multiple variables  \n",
    "    df_gptest = df[['drive-wheels','body-style','price']]  \n",
    "    grouped_test1 = df_gptest.groupby(['drive-wheels','body-style'],as_index=False).mean()  \n",
    "    grouped_test1  \n",
    "14. Group into a pivot  \n",
    "    grouped_pivot = grouped_test1.pivot(index='drive-wheels',columns='body-style')  \n",
    "    grouped_pivot  \n",
    "15. Heatmap  \n",
    "    plt.pcolor(grouped_pivot, cmap='RdBu')  \n",
    "    plt.colorbar()  \n",
    "    plt.show()\n",
    "16. Heatmap - fine tuned  \n",
    "    fig, ax = plt.subplots()  \n",
    "    im = ax.pcolor(grouped_pivot, cmap='RdBu')  \n",
    "    #label names  \n",
    "    row_labels = grouped_pivot.columns.levels[1]  \n",
    "    col_labels = grouped_pivot.index  \n",
    "    #move ticks and labels to the center  \n",
    "    ax.set_xticks(np.arange(grouped_pivot.shape[1]) + 0.5, minor=False)  \n",
    "    ax.set_yticks(np.arange(grouped_pivot.shape[0]) + 0.5, minor=False)  \n",
    "    #insert labels  \n",
    "    ax.set_xticklabels(row_labels, minor=False)  \n",
    "    ax.set_yticklabels(col_labels, minor=False)  \n",
    "    #rotate label if too long  \n",
    "    plt.xticks(rotation=90)  \n",
    "    fig.colorbar(im)  \n",
    "    plt.show()\n",
    "17. Pearson-Correlation  \n",
    "    from scipy import stats  \n",
    "    pearson_coef, p_value = stats.pearsonr(df['wheel-base'], df['price'])  \n",
    "    print(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)  \n",
    "18. ANOVA  \n",
    "    grouped_test2=df_gptest[['drive-wheels', 'price']].groupby(['drive-wheels'])  \n",
    "    f_val, p_val = stats.f_oneway(grouped_test2.get_group('fwd')['price'], grouped_test2.get_group('rwd')['price'], grouped_test2.get_group('4wd')['price'])  \n",
    "    print( \"ANOVA results: F=\", f_val, \", P =\", p_val) \n",
    "\n",
    "F-test score: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. A larger score means there is a larger difference between the means.\n",
    "\n",
    "P-value: P-value tells how statistically significant is our calculated score value.\n",
    "\n",
    "[Home](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='mod'></a>\n",
    "# Model Development\n",
    "A model or estimator can be thought of as a \n",
    "* mathematical equation used to predict the value given one or more other values\n",
    "* Relating one or more independent variables or features to dependent variables.\n",
    "\n",
    "You'll learn about \n",
    "* simple and multiple linear regression, \n",
    "* model evaluation using visualization, \n",
    "* polynomial regression and pipelines, \n",
    "* R-squared and MSE for in-sample evaluation, \n",
    "* prediction and decision making, and how you can determine a fair value\n",
    "\n",
    "### Linear Regression\n",
    "Linear regression will refer to one independent variable to make a prediction. Multiple linear regression will refer to multiple independent variables to make a prediction. Simple linear regression (SLR) is a method to help us understand the relationship between two variables\n",
    "* The predictor independent variable $x$ and \n",
    "* the target dependent variable $y$.\n",
    "\n",
    "$ y = b_0 + b_1 x$\n",
    "\n",
    "$b_0$: the interceptor  \n",
    "$b_1$: the slope  \n",
    "\n",
    "In order to determine the line, we take data points from our data set marked in red here. We then use these training points to fit our model. The results of the training points are the parameters. We usually store the data points into data frame or numpy arrays. \n",
    "The value we would like to predict is called the target that we store in the <b>array y</b>. We store the dependent variable in the data frame or <b>array x</b>. Each sample corresponds to a different row in each data frame or array.  \n",
    "\n",
    "We have a set of training points. We use these training points to fit or train the model and get parameters. We then use these parameters in the model. We now have a model. We use the $\\hat{}$ on the $\\hat{y}$ to denote the model is an estimate. We can use this model to predict values that we haven't seen.\n",
    "\n",
    "$ \\hat{y} = b_0 + b_1 x$  \n",
    "\n",
    "To fit the model in Python, first we import \n",
    "1. linear_model from sklearn  \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "2. then create a linear regression object using the constructor  \n",
    "    lm = LinearRegression()\n",
    "3. Define the predictor and target variable\n",
    "    x = df[['x-variables']] \n",
    "    y = df['y-variable']\n",
    "4. Use the method fit to fit the model and find the parameters $b_0$ and $b_1$.  \n",
    "    lm.fit(x,y)  \n",
    "5. We can obtain prediction using the method predict. The output is an array. The array has the same number of samples as the input x. The intercept $b_0$ is an attribute of the object lm. The slope $b_1$ is also an attribute of the object lm.  \n",
    "    $ \\hat{y}$ = lm.predict(x)  \n",
    "\n",
    "### Multiple Regression\n",
    "Multiple linear regression (MLR) is used to explain the relationship between one continuous target y variable and two or more predictor x variables. If we have for example 4 predictor variables then \n",
    "* $b_0$ intercept x = zero \n",
    "* $b_1$ the coefficient or parameter of $x_1$\n",
    "* $b_2$ the coefficient of parameter $x_2$ and so on\n",
    "\n",
    "$ \\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + ...$  \n",
    "\n",
    "1. extract the Var1-n predictor variables and store them in the variable z  \n",
    "    Z = df[['VarX1','VarX2','VarX3']]  \n",
    "2. train the model using the method train with the features or dependent variables \n",
    "    lm.fit(Z, df['Y'])\n",
    "3. Obtain a prediction  \n",
    "    $ \\hat{y}$ = lm.predict(X)  \n",
    "\n",
    "### Model Evaluation Using Visualisation\n",
    "Regression plots are a good estimate of the relationship between two variables, the strength of the correlation, and the direction of the relationship (positive or negative). The horizontal axis is the independent variable. The vertical axis is the dependent variable. Each point represents a different target point. The fitted line represents the predicted value. There are several ways to plot a regression plot.\n",
    "\n",
    "<b>Regression plot</b>\n",
    "1. Import <b>seaborn<b>  \n",
    "    import seaborn as sns  \n",
    "2. Then use the \"regplot\" function. The parameter $x$ is the name of the column that contains the independent variable or feature. The parameter $y$, contains the name of the column that contains the name of the dependent variable or target. The parameter data is the name of the dataframe. The result is given by the plot.  \n",
    "    sns.regplot(x='xVar',y='yVar',data=df)\n",
    "    plt.ylim(0,)  \n",
    "\n",
    "<b>Residual plot</b>\n",
    "The difference between the observed value of the dependent variable ($y$) and the predicted value ($ \\hat{y}$) is called the residual (e). Each data point has one residual.  \n",
    "    $e = y - \\hat{y} $ \n",
    "\n",
    "Both the <b>sum</b> and the <b>mean</b> of the residuals are equal to zero. That is, $\\sum e = 0$ and $ \\overline e = 0.$  \n",
    "A residual plot is a graph that shows the residuals on the vertical axis and the independent variable on the horizontal axis. If the points in a <b>residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate</b> for the data; otherwise, a nonlinear model is more appropriate.\n",
    "\n",
    "The residual plot represents the error between the actual value. Examining the predicted value and actual value we see a difference. We obtain that value by subtracting the predicted value, and the actual target value.  \n",
    "1.  Import <b>seaborn</b>  \n",
    "    import seaborn as sns  \n",
    "2. The first parameter is a series of dependent variable or feature. The second parameter is a series of dependent variable or target. We see in this case, the residuals have a curvature.  \n",
    "    sns.residplot(df['VarX1'], df['Target'])\n",
    "\n",
    "<b>Distribution plot</b>  \n",
    "How do we visualize a model for Multiple Linear Regression? One way to look at the fit of the model is by looking at the <b>distribution plot</b>: We can look at the distribution of the fitted values that result from the model and compare it to the distribution of the actual values.  \n",
    "![Residual plot](https://static-01.hindawi.com/articles/jat/volume-2020/8953182/figures/8953182.fig.0015.svgz)\n",
    "\n",
    "A distribution plot counts the predicted value versus the actual value. We examined the vertical axis. We then count and plot the number of predicted points that are approximately equal to one.  \n",
    "The values of the targets and predicted values are continuous. A histogram is for discrete values. Therefore, pandas will convert them to a distribution. The vertical axis is scaled to make the area under the distribution equal to one.\n",
    "    \n",
    "### Polynomial Regression and Pipelines\n",
    "Polynomial Regression is a form of linear regression in which the relationship between the independent variable $x$ and dependent variable $y$ is modeled as an $n^{th}$ degree polynomial. Polynomial regression is a special case of the general linear regression.  \n",
    "\n",
    "This method is beneficial for describing curvilinear relationships which is what you get by squaring(^2) or setting higher order terms of the predictor variables in the model transforming the data.  \n",
    "\n",
    "The model can be quadratic, which means that the predictor variable in the model is squared. We use a bracket to indicate it as an exponent. \n",
    "* The model can be quadratic - $2^nd$ order  \n",
    "    $ \\hat{y} = b_0 + b_1 x_1 + b_2 (x_1)^2$  \n",
    "* The model can be cubic - $3^rd$ order  \n",
    "    $ \\hat{y} = b_0 + b_1 x_1 + b_2 (x_1)^2 + b_3 (x_1)^3$ \n",
    "* There also exists <b>higher order polynomial regressions</b>  \n",
    "    $ \\hat{y} = b_0 + b_1 x_1 + b_2 (x_1)^2 + b_3 (x_1)^3 + ...$ \n",
    "\n",
    "in Python  \n",
    "    f = np.polyfit(x,y,3)  \n",
    "    p = np.polyld(f)  \n",
    "    print(p)  \n",
    "\n",
    "    \n",
    "### Pipline  \n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a ‘__’.  \n",
    "Data Pipelines simplify the steps of processing the data. We use the module Pipeline to create a pipeline. We also use StandardScaler as a step in our pipeline.  \n",
    "We create the pipeline, by creating a list of tuples including the name of the model or estimator and its corresponding constructor.  \n",
    "    Input=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model',LinearRegression())]\n",
    "\n",
    "[Home](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='insamp'></a>\n",
    "# Measures for In-Sample Evaluation\n",
    "In-sample evaluation tells us how well our model fits the data already given to train it. It does not give us an estimate of how well the train model can predict new data. \n",
    "\n",
    "We want to numerically evaluate our models. Let’s look at some of the measures that we use for in-sample evaluation. These measures are a way to numerically determine how good the model fits on our data. Two important measures that we often use to determine the fit of a model are: \n",
    "* Mean Square Error (MSE)\n",
    "* R-squared\n",
    "\n",
    "<b>MSE</b>  \n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(df['VarX'],Y_predict_simple_fit)\n",
    "\n",
    "<b>$R^2$</b>  \n",
    "R-squared is also called the coefficient of determination. It’s a measure to determine how close the data is to the fitted regression line.\n",
    "X = df[['xVar']]  \n",
    "Y = df['yVar']  \n",
    "lm.fit(X,Y)  \n",
    "lm.score(X,Y)  \n",
    "\n",
    "# Predicting and Decision taking\n",
    "How can we determine if our model is correct? The first thing you should do is make sure your model results make sense. You should always use:  \n",
    "* visualization  \n",
    "* numerical measures for evaluation and  \n",
    "* comparing between different models  \n",
    "\n",
    "<b>Exercise (SLR)</b>  \n",
    "1. Import libraries  \n",
    "    import pandas as pd  \n",
    "    import numpy as np  \n",
    "    import matplotlib.pyplot as plt  \n",
    "2. Build dataframe  \n",
    "    path = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/automobileEDA.csv'  \n",
    "    df = pd.read_csv(path)  \n",
    "    df.head()\n",
    "3. Load module for linear regression\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "4. Create liner regression object\n",
    "    lm = LinearRegression()  \n",
    "    lm  \n",
    "5. Define the variables  \n",
    "    X = df[['highway-mpg']]  \n",
    "    Y = df['price']\n",
    "6. Fit the linear model  \n",
    "    lm.fit(X,Y)\n",
    "7. Output prediction  \n",
    "    Yhat=lm.predict(X)  \n",
    "    Yhat[0:5]   \n",
    "8. Intercept: lm.intercept_  \n",
    "    Slope: lm.coef_  \n",
    "\n",
    "<b>Exercise (MLR)</b>  \n",
    "1. Define the variable  \n",
    "    Z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]  \n",
    "2. Fit the model  \n",
    "    lm.fit(Z, df['price'])\n",
    "3. Intercept: lm.intercept_\n",
    "    Values of coefficients b1 ... bn  \n",
    "    lm.coef_  \n",
    "4. Visualisation  \n",
    "    import seaborn as sns  \n",
    "      \n",
    "    %matplotlib inline  \n",
    "    width = 12  \n",
    "    height = 10  \n",
    "    plt.figure(figsize=(width, height))  \n",
    "    sns.regplot(x=\"highway-mpg\", y=\"price\", data=df)  \n",
    "    plt.ylim(0,)  \n",
    "\n",
    "<b>Residual plot</b>   \n",
    "width = 12  \n",
    "height = 10  \n",
    "plt.figure(figsize=(width, height))  \n",
    "sns.residplot(df['highway-mpg'], df['price'])  \n",
    "plt.show()  \n",
    "\n",
    "What do we pay attention to when looking at a residual plot? We look at the spread of the residuals: If the points in a residual plot are randomly spread out around the x-axis, then a linear model is appropriate for the data. Why is that? Randomly spread out residuals means that the variance is constant, and thus the linear model is a good fit for this data.\n",
    "\n",
    "<b>Distribution plot</b>  \n",
    "Y_hat = lm.predict(Z)  \n",
    "plt.figure(figsize=(width, height))  \n",
    "  \n",
    "ax1 = sns.distplot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")  \n",
    "sns.distplot(Yhat, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)  \n",
    "  \n",
    "plt.title('Actual vs Fitted Values for Price')  \n",
    "plt.xlabel('Price (in dollars)')  \n",
    "plt.ylabel('Proportion of Cars')  \n",
    "  \n",
    "plt.show()  \n",
    "plt.close()  \n",
    "\n",
    "<b>Distribution plot</b>  \n",
    "def PlotPolly(model, independent_variable, dependent_variabble, Name):  \n",
    "    x_new = np.linspace(15, 55, 100)  \n",
    "    y_new = model(x_new)  \n",
    "\n",
    "    plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')  \n",
    "    plt.title('Polynomial Fit with Matplotlib for Price ~ Length')  \n",
    "    ax = plt.gca()  \n",
    "    ax.set_facecolor((0.898, 0.898, 0.898))  \n",
    "    fig = plt.gcf()  \n",
    "    plt.xlabel(Name)  \n",
    "    plt.ylabel('Price of Cars')  \n",
    "  \n",
    "    plt.show()  \n",
    "    plt.close()  \n",
    "    \n",
    "x = df['highway-mpg']\n",
    "y = df['price']\n",
    "  \n",
    "f = np.polyfit(x, y, 3)  \n",
    "p = np.poly1d(f)  \n",
    "print(p)  \n",
    "\n",
    "PlotPolly(p, x, y, 'highway-mpg')  \n",
    "\n",
    "np.polyfit(x, y, 3)  \n",
    "\n",
    "<b>Pipeline</b>  \n",
    "from sklearn.pipeline import Pipeline  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "Input=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model',LinearRegression())]  \n",
    "\n",
    "pipe=Pipeline(Input)  \n",
    "pipe  \n",
    "\n",
    "pipe.fit(Z,y)  \n",
    "\n",
    "ypipe=pipe.predict(Z)  \n",
    "ypipe[0:4]  \n",
    "\n",
    "<b>Measures for In-Sample Evaluation</b>  \n",
    "Two very important measures that are often used in Statistics to determine the accuracy of a model are:  \n",
    "\n",
    "* R^2 / R-squared\n",
    "* Mean Squared Error (MSE\n",
    "\n",
    "\n",
    "### Find the R^2\n",
    "lm.fit(X, Y)\n",
    "print('The R-square is: ', lm.score(X, Y))\n",
    "\n",
    "### MSE\n",
    "Yhat=lm.predict(X)\n",
    "print('The output of the first four predicted value is: ', Yhat[0:4])  \n",
    "\n",
    "### Decision Making: Determining a Good Model Fit  \n",
    "Now that we have visualized the different models, and generated the R-squared and MSE values for the fits, how do we determine a good model fit?\n",
    "\n",
    "#### What is a good R-squared value?\n",
    "When comparing models, the model with the higher R-squared value is a better fit for the data.\n",
    "\n",
    "#### What is a good MSE?\n",
    "When comparing models, the model with the smallest MSE value is a better fit for the data.\n",
    "\n",
    "Let's take a look at the values for the different models.\n",
    "Simple Linear Regression: Using Highway-mpg as a Predictor Variable of Price.\n",
    "\n",
    "R-squared: 0.49659118843391759\n",
    "MSE: 3.16 x10^7\n",
    "Multiple Linear Regression: Using Horsepower, Curb-weight, Engine-size, and Highway-mpg as Predictor Variables of Price.\n",
    "\n",
    "R-squared: 0.80896354913783497\n",
    "MSE: 1.2 x10^7\n",
    "Polynomial Fit: Using Highway-mpg as a Predictor Variable of Price.\n",
    "\n",
    "R-squared: 0.6741946663906514\n",
    "MSE: 2.05 x 10^7\n",
    "Simple Linear Regression model (SLR) vs Multiple Linear Regression model (MLR)\n",
    "Usually, the more variables you have, the better your model is at predicting, but this is not always true. Sometimes you may not have enough data, you may run into numerical problems, or many of the variables may not be useful and or even act as noise. As a result, you should always check the MSE and R^2.\n",
    "\n",
    "So to be able to compare the results of the MLR vs SLR models, we look at a combination of both the R-squared and MSE to make the best conclusion about the fit of the model.\n",
    "\n",
    "MSEThe MSE of SLR is 3.16x10^7 while MLR has an MSE of 1.2 x10^7. The MSE of MLR is much smaller.\n",
    "R-squared: In this case, we can also see that there is a big difference between the R-squared of the SLR and the R-squared of the MLR. The R-squared for the SLR (~0.497) is very small compared to the R-squared for the MLR (~0.809).\n",
    "This R-squared in combination with the MSE show that MLR seems like the better model fit in this case, compared to SLR.\n",
    "\n",
    "#### Simple Linear Model (SLR) vs Polynomial Fit\n",
    "MSE: We can see that Polynomial Fit brought down the MSE, since this MSE is smaller than the one from the SLR.\n",
    "R-squared: The R-squared for the Polyfit is larger than the R-squared for the SLR, so the Polynomial Fit also brought up the R-squared quite a bit.\n",
    "Since the Polynomial Fit resulted in a lower MSE and a higher R-squared, we can conclude that this was a better fit model than the simple linear regression for predicting Price with Highway-mpg as a predictor variable.\n",
    "\n",
    "#### Multiple Linear Regression (MLR) vs Polynomial Fit\n",
    "MSE: The MSE for the MLR is smaller than the MSE for the Polynomial Fit.\n",
    "R-squared: The R-squared for the MLR is also much larger than for the Polynomial Fit.\n",
    "\n",
    "[Home](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='eva'></a>\n",
    "# Model Evaluation\n",
    "Model evaluation tells us how our model performs in the real world.\n",
    "\n",
    "Separating data into training and testing sets is an important part of model evaluation. \n",
    "\n",
    "We use training set to build a model and discover predictive relationships. We then use a testing set to evaluate model performance. When we have completed testing our model, we should use all the data to train the model. \n",
    "\n",
    "A popular function, in the scikit-learn package for splitting datasets, is the train test split function. This function randomly splits a dataset into training and testing subsets. \n",
    "\n",
    "imported from sklearn.cross-validation. The input parameters y_data is the target variable. In the car appraisal example, it would be the price and x_data, the list of predictive variables. In this case, it would be all the other variables in the car dataset that we are using to try to predict the price. The output is an array. x_train and y_train the subsets for training. x_test and y_test the subsets for testing.\n",
    "\n",
    "cross-validation. One of the most common out of sample evaluation metrics is cross-validation. In this method, the dataset is split into K equal groups. Each group is referred to as a fold. For example, four folds. Some of the folds can be used as a training set which we use to train the model and the remaining parts are used as a test set, which we use to test the model.\n",
    "\n",
    "cross_ val_predict function. The input parameters are exactly the same as the cross_val_score function, but the output is a prediction\n",
    "\n",
    "## Overfitting and Underfitting\n",
    "The goal of Model Selection is to determine the order of the polynomial to provide the best estimate of the function y(x).\n",
    "\n",
    "* Underfitting: where the model is too simple to fit the data. \n",
    "* Overfitting: where the model is too flexible and fits the noise rather than the function. \n",
    "\n",
    "plot of the mean square error for the training and testing set of different order polynomials. The horizontal axis represents the order of the polynomial. The vertical axis is the mean square error. The training error decreases with the order of the polynomial. The test error is a better means of estimating the error of a polynomial. The error decreases 'til the best order of the polynomial is determined. Then the error begins to increase. We select the order that minimizes the test error.\n",
    "\n",
    "## Ridge Regression\n",
    "Ridge regression prevents overfitting. We will focus on polynomial regression for visualization, but overfitting is also a big problem when you have multiple independent variables, or features.\n",
    "\n",
    "Ridge regression controls the magnitude of these polynomial coefficients by introducing the parameter alpha. Alpha is a parameter we select before fitting or training the model. Each row in the following table represents an increasing value of alpha.\n",
    "\n",
    "To make a prediction using ridge regression, import ridge from sklearn.linear_models. Create a ridge object using the constructor. The parameter alpha is one of the arguments of the constructor. We train the model using the fit method. To make a prediction, we use the predict method. In order to determine the parameter alpha, we use some data for training.\n",
    "\n",
    "## Gride Search\n",
    "Scikit-learn has a means of automatically iterating over these hyperparameters using cross-validation. This method is called Grid Search. Grid Search takes the model or objects you would like to train and different values of the hyperparameters. It then calculates the mean square error or R-squared for various hyperparameter values, allowing you to choose the best values.\n",
    "\n",
    "<b>Exercise</b>\n",
    "1. Perp-work\n",
    "    import pandas as pd  \n",
    "    import numpy as np\n",
    "2. Import clean data  \n",
    "    path = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/module_5_auto.csv'  \n",
    "    df = pd.read_csv(path)  \n",
    "    df.to_csv('module_5_auto.csv')\n",
    "    df=df._get_numeric_data()  \n",
    "    df.head()  \n",
    "3. Install libraries for plotting\n",
    "    %%capture  \n",
    "    ! pip install ipywidgets\n",
    "    \n",
    "    from IPython.display import display  \n",
    "    from IPython.html import widgets  \n",
    "    from IPython.display import display  \n",
    "    from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "4. Defining functions for plotting  \n",
    "    def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):  \n",
    "    width = 12  \n",
    "    height = 10  \n",
    "    plt.figure(figsize=(width, height))  \n",
    "    ax1 = sns.distplot(RedFunction, hist=False, color=\"r\", label=RedName)  \n",
    "    ax2 = sns.distplot(BlueFunction, hist=False, color=\"b\", label=BlueName, ax=ax1)  \n",
    "    plt.title(Title)  \n",
    "    plt.xlabel('Price (in dollars)')  \n",
    "    plt.ylabel('Proportion of Cars')  \n",
    "    plt.show()  \n",
    "    plt.close()  \n",
    "    \n",
    "    def PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):  \n",
    "    width = 12  \n",
    "    height = 10  \n",
    "    plt.figure(figsize=(width, height))  \n",
    "    #training data  \n",
    "    #testing data  \n",
    "    # lr:  linear regression object  \n",
    "    #poly_transform:  polynomial transformation object  \n",
    "    xmax=max([xtrain.values.max(), xtest.values.max()])  \n",
    "    xmin=min([xtrain.values.min(), xtest.values.min()])  \n",
    "    x=np.arange(xmin, xmax, 0.1)  \n",
    "    plt.plot(xtrain, y_train, 'ro', label='Training Data')  \n",
    "    plt.plot(xtest, y_test, 'go', label='Test Data')  \n",
    "    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')  \n",
    "    plt.ylim([-10000, 60000])  \n",
    "    plt.ylabel('Price')  \n",
    "    plt.legend()\n",
    "5. Training and Testing\n",
    "    Split into training and testing data\n",
    "        y_data = df['price']  \n",
    "        x_data=df.drop('price',axis=1)  \n",
    "        \n",
    "        from sklearn.model_selection import train_test_split  \n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=1)  \n",
    "        print(\"number of test samples :\", x_test.shape[0])  \n",
    "        print(\"number of training samples:\",x_train.shape[0])\n",
    "6. Import the LinearRegression\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lre=LinearRegression() #create linear regression object\n",
    "    lre.fit(x_train[['horsepower']], y_train) # fit the model\n",
    "7. Caluculate the R^2  \n",
    "    lre.score(x_test[['horsepower']], y_test)  #on the test data\n",
    "    lre.score(x_train[['horsepower']], y_train)  # on the training data\n",
    "\n",
    "<b>Cross-validation Score</b>\n",
    "from sklearn.model_selection import cross_val_score  \n",
    "Rcross = cross_val_score(lre, x_data[['horsepower']], y_data, cv=4)  \n",
    "print(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())  \n",
    "-1 * cross_val_score(lre,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')  \n",
    "\n",
    "Using corss-val-predict  \n",
    "from sklearn.model_selection import cross_val_predict  \n",
    "yhat = cross_val_predict(lre,x_data[['horsepower']], y_data,cv=4)\n",
    "yhat[0:5]\n",
    "\n",
    "<b>Overfitting, Underfitting and Model selection</b>  \n",
    "1. Create a multiple linear regression object   \n",
    "    lr = LinearRegression()  \n",
    "    lr.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)  \n",
    "2. Predict using training data  \n",
    "    yhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])  \n",
    "    yhat_train[0:5]\n",
    "3. Import libraries  \n",
    "    import matplotlib.pyplot as plt  \n",
    "    %matplotlib inline  \n",
    "    import seaborn as sns  \n",
    "4. Examine the distribution from the training set: \n",
    "    Title = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'  \n",
    "    DistributionPlot(y_train, yhat_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", Title)\n",
    "5. Examine the distribution from the testing set: \n",
    "    Title = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'  \n",
    "    DistributionPlot(y_test, yhat_test, \"Actual Values (Train)\", \"Predicted Values (Test)\", Title)\n",
    "6. Overfitting  \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)  \n",
    "    pr = PolynomialFeatures(degree=5)  \n",
    "    x_train_pr = pr.fit_transform(x_train[['horsepower']])  \n",
    "    x_test_pr = pr.fit_transform(x_test[['horsepower']])  \n",
    "    pr  \n",
    "    poly = LinearRegression()  \n",
    "    poly.fit(x_train_pr, y_train)  \n",
    "    yhat = poly.predict(x_test_pr)  \n",
    "    yhat[0:5]  \n",
    "    print(\"Predicted values:\", yhat[0:4])  \n",
    "    print(\"True values:\", y_test[0:4].values)  \n",
    "    PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train, y_test, poly,pr)  \n",
    "    poly.score(x_train_pr, y_train) # .557  \n",
    "    #The lower the R^2, the worse the model, a Negative R^2 is a sign of overfitting.\n",
    "      \n",
    "    R^2 test  \n",
    "    Rsqu_test = []  \n",
    "    order = [1, 2, 3, 4]  \n",
    "    \n",
    "    for n in order:  \n",
    "        pr = PolynomialFeatures(degree=n)  \n",
    "        x_train_pr = pr.fit_transform(x_train[['horsepower']])  \n",
    "        x_test_pr = pr.fit_transform(x_test[['horsepower']])    \n",
    "        lr.fit(x_train_pr, y_train)  \n",
    "        Rsqu_test.append(lr.score(x_test_pr, y_test))\n",
    "        plt.plot(order, Rsqu_test)  \n",
    "        plt.xlabel('order')  \n",
    "        plt.ylabel('R^2')  \n",
    "        plt.title('R^2 Using Test Data')  \n",
    "        plt.text(3, 0.75, 'Maximum R^2 ')\n",
    "\n",
    "<b>Ridge regression</b>\n",
    "\n",
    "\n",
    "<b>Grid Search</b>\n",
    "\n",
    "\n",
    "[Home](#home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
